{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smGcoXqQM7Z1"
      },
      "source": [
        "# INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q9GSS9AMCUHm"
      },
      "outputs": [],
      "source": [
        "# @title MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import datetime\n",
        "\n",
        "dt_now = datetime.datetime.now()\n",
        "\n",
        "print(dt_now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ww6iS-woCc9h"
      },
      "outputs": [],
      "source": [
        "# @title diffusers ver 2.4 install\n",
        "%pip install torch==2.4.1 diffusers[torch]==0.25.0 huggingface_hub==0.25.2 torchvision==0.19.1 transformers==4.45.2 compel==2.0.2 omegaconf==2.4.0.dev1 pytorch-lightning accelerate safetensors==0.4.1 controlnet-aux==0.0.7 xformers==0.0.28\n",
        "%apt-get install aria2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MbNxxlCxCptm"
      },
      "outputs": [],
      "source": [
        "# @title MODEL DL\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "!mkdir /content/models\n",
        "!mkdir /content/vae\n",
        "\n",
        "# @markdown token\n",
        "civitai_token = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown vae\n",
        "vae_url = \"\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown model1\n",
        "model1_url = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown model2\n",
        "model2_url = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown model3\n",
        "model3_url = \"\" # @param {type:\"string\"}\n",
        "\n",
        "if civitai_token != \"\":\n",
        "  token_surfix = \"&token=\" + civitai_token\n",
        "  vae_url + token_surfix\n",
        "  model1_url_dl = model1_url + token_surfix\n",
        "  model2_url_dl = model2_url + token_surfix\n",
        "  model3_url_dl = model3_url  + token_surfix\n",
        "\n",
        "!aria2c -x3 --no-overwrite \"{vae_url}\" -d \"/content/vae/\"\n",
        "!aria2c -x3 --no-overwrite \"{model1_url_dl}\" -d \"/content/models/\"\n",
        "!aria2c -x3 --no-overwrite \"{model2_url_dl}\" -d \"/content/models/\"\n",
        "!aria2c -x3 --no-overwrite \"{model3_url_dl}\" -d \"/content/models/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rcami2LjMvoQ"
      },
      "outputs": [],
      "source": [
        "# @title IMPORT\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DiffusionPipeline,\n",
        "    DDIMScheduler,\n",
        "    DDPMScheduler,\n",
        "    DEISMultistepScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    UniPCMultistepScheduler,\n",
        ")\n",
        "from diffusers.models import AutoencoderKL\n",
        "from diffusers import StableDiffusionLatentUpscalePipeline\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from compel import Compel, DiffusersTextualInversionManager\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "def get_latest_file(directory):\n",
        "    # ディレクトリ内の全ファイルのリストを取得\n",
        "    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    if not files:\n",
        "        return None  # ファイルがない場合はNoneを返す\n",
        "\n",
        "    # 最も新しいファイルを取得（最終更新時刻が最新）\n",
        "    latest_file = max(files, key=os.path.getmtime)\n",
        "\n",
        "    return latest_file\n",
        "\n",
        "\n",
        "model_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "!mkdir \"./cache\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "vae = AutoencoderKL.from_single_file(get_latest_file(\"/content/vae/\"))\n",
        "\n",
        "SCHEDULERS_LIST = {\n",
        "    'DDIM' : DDIMScheduler,\n",
        "    'DDPM' : DDPMScheduler,\n",
        "    'DEISMultistep' : DEISMultistepScheduler,\n",
        "    'DPMSolverMultistep' : DPMSolverMultistepScheduler,\n",
        "    'DPMSolverSinglestep' : DPMSolverSinglestepScheduler,\n",
        "    'EulerAncestralDiscrete' : EulerAncestralDiscreteScheduler,\n",
        "    'EulerDiscrete' : EulerDiscreteScheduler,\n",
        "    'HeunDiscrete' : HeunDiscreteScheduler,\n",
        "    'KDPM2AncestralDiscrete' : KDPM2AncestralDiscreteScheduler,\n",
        "    'KDPM2Discrete' : KDPM2DiscreteScheduler,\n",
        "    'UniPCMultistep' : UniPCMultistepScheduler\n",
        "}\n",
        "####################\n",
        "###### compel ######\n",
        "####################\n",
        "\n",
        "def concat_tensor(t):\n",
        "    t_list = torch.split(t, 1, dim=0)\n",
        "    t = torch.cat(t_list, dim=1)\n",
        "    return t\n",
        "\n",
        "\n",
        "def detokenize(chunk, actual_prompt):\n",
        "    chunk[-1] = chunk[-1].replace('</w>', '')\n",
        "    chanked_prompt = ''.join(chunk).strip()\n",
        "    while '</w>' in chanked_prompt:\n",
        "        if actual_prompt[chanked_prompt.find('</w>')] == ' ':\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', ' ', 1)\n",
        "        else:\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', '', 1)\n",
        "    actual_prompt = actual_prompt.replace(chanked_prompt,'')\n",
        "    return chanked_prompt.strip(), actual_prompt.strip()\n",
        "\n",
        "def tokenize_line(line, tokenizer): # split into chunks\n",
        "    actual_prompt = line.lower().strip()\n",
        "    actual_tokens = tokenizer.tokenize(actual_prompt)\n",
        "    max_tokens = tokenizer.model_max_length - 2\n",
        "    separators = {\n",
        "        'comma': tokenizer.tokenize(',')[0],\n",
        "        'dot': tokenizer.tokenize('.')[0],\n",
        "        'colon': tokenizer.tokenize(':')[0]\n",
        "    }\n",
        "\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "    for item in actual_tokens:\n",
        "        chunk.append(item)\n",
        "        if len(chunk) == max_tokens:\n",
        "            if chunk[-1] not in list(separators.values()):\n",
        "                for i in range(max_tokens-1, -1, -1):\n",
        "                    if chunk[i] in list(separators.values()):\n",
        "                        actual_chunk, actual_prompt = detokenize(chunk[:i+1], actual_prompt)\n",
        "                        chunks.append(actual_chunk)\n",
        "                        chunk = chunk[i+1:]\n",
        "                        break\n",
        "                else:\n",
        "                    actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                    chunks.append(actual_chunk)\n",
        "                    chunk = []\n",
        "            else:\n",
        "                actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                chunks.append(actual_chunk)\n",
        "                chunk = []\n",
        "    if chunk:\n",
        "        actual_chunk, _ = detokenize(chunk, actual_prompt)\n",
        "        chunks.append(actual_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def token_auto_concat_embeds(pipe, positive, negative):\n",
        "    max_length = pipe.tokenizer.model_max_length\n",
        "    positive = separate_prompt(positive)\n",
        "    negative = separate_prompt(negative)\n",
        "    positive_length = pipe.tokenizer(positive, return_tensors=\"pt\").input_ids.shape[-1]\n",
        "    negative_length = pipe.tokenizer(negative, return_tensors=\"pt\").input_ids.shape[-1]\n",
        "\n",
        "    print(f'Token length is model maximum: {max_length}, positive length: {positive_length}, negative length: {negative_length}.')\n",
        "    if max_length < positive_length or max_length < negative_length:\n",
        "        print('Concatenated embedding.')\n",
        "        if positive_length > negative_length:\n",
        "            positive_ids = pipe.tokenizer(positive, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "            negative_ids = pipe.tokenizer(negative, truncation=False, padding=\"max_length\", max_length=positive_ids.shape[-1], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "        else:\n",
        "            negative_ids = pipe.tokenizer(negative, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "            positive_ids = pipe.tokenizer(positive, truncation=False, padding=\"max_length\", max_length=negative_ids.shape[-1],  return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    else:\n",
        "        positive_ids = pipe.tokenizer(positive, truncation=False, padding=\"max_length\", max_length=max_length,  return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "        negative_ids = pipe.tokenizer(negative, truncation=False, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "    positive_concat_embeds = []\n",
        "    negative_concat_embeds = []\n",
        "    for i in range(0, positive_ids.shape[-1], max_length):\n",
        "        positive_concat_embeds.append(pipe.text_encoder(positive_ids[:, i: i + max_length])[0])\n",
        "        negative_concat_embeds.append(pipe.text_encoder(negative_ids[:, i: i + max_length])[0])\n",
        "\n",
        "    positive_prompt_embeds = torch.cat(positive_concat_embeds, dim=1)\n",
        "    negative_prompt_embeds = torch.cat(negative_concat_embeds, dim=1)\n",
        "    return positive_prompt_embeds, negative_prompt_embeds\n",
        "\n",
        "\n",
        "def get_prompt_with_weight(pipe, prompt, negative):\n",
        "  textual_inversion_manager = DiffusersTextualInversionManager(pipe)\n",
        "\n",
        "  compel_proc = Compel(\n",
        "      tokenizer=pipe.tokenizer,\n",
        "      text_encoder=pipe.text_encoder,\n",
        "      textual_inversion_manager=textual_inversion_manager,\n",
        "      truncate_long_prompts=False)\n",
        "\n",
        "  #prompt, negative = form_prompt_length(prompt, negative)\n",
        "\n",
        "  prompt_formed = separate_prompt(prompt)\n",
        "  negative_formed = separate_prompt(negative)\n",
        "\n",
        "  print(\"formed prompt:\")\n",
        "  print(prompt_formed)\n",
        "  print(\"formed negative:\")\n",
        "  print(negative_formed)\n",
        "\n",
        "  #prompt_embeds = compel_proc([prompt_formed])\n",
        "  #negative_prompt_embeds = compel_proc([negative_formed])\n",
        "  prompt_embeds = compel_proc.build_conditioning_tensor(prompt_formed)\n",
        "  negative_prompt_embeds = compel_proc.build_conditioning_tensor(negative_formed)\n",
        "\n",
        "  prompt_embeds, negative_prompt_embeds = compel_proc.pad_conditioning_tensors_to_same_length([prompt_embeds, negative_prompt_embeds])\n",
        "\n",
        "  return prompt_embeds, negative_prompt_embeds\n",
        "\n",
        "def form_prompt_length(posi, neg):\n",
        "  # posi_chunks = posi.split(\",\")\n",
        "  # neg_chunks = neg.split(\",\")\n",
        "\n",
        "  gap = len(posi) - len(neg)\n",
        "  if gap > 0:\n",
        "    for i in range(0,gap):\n",
        "      neg += \",\"\n",
        "  elif  gap < 0:\n",
        "    for i in range(0, abs(gap)):\n",
        "      posi += \",\"\n",
        "\n",
        "  return posi, neg\n",
        "\n",
        "def separate_prompt(prompt):\n",
        "  #prompts = prompt.split(\",\")\n",
        "  prompts = split_prompt(prompt)\n",
        "  result = \"\"\n",
        "  for p in prompts:\n",
        "    word = replace_weight(p)\n",
        "    result += word + \",\"\n",
        "  if result[-1] == \",\":\n",
        "    result = result[:len(result) - 1]\n",
        "  print(\"separate prompt:\")\n",
        "  print(result)\n",
        "  return result\n",
        "\n",
        "def replace_weight(word):\n",
        "  pattern = r\"\\(.*\\)\"\n",
        "  #s = trim_parenthesis_space(word)\n",
        "  # マッチオブジェクトの取得\n",
        "  matched = re.search(pattern, word)\n",
        "\n",
        "  if matched:\n",
        "    s, weight = get_word_weight(word)\n",
        "    s = modify_string(s, weight)\n",
        "\n",
        "    return s\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "\n",
        "def modify_string(s, weight):\n",
        "    # 数字が1よりも大きかったら\n",
        "    if weight == 0:\n",
        "      return \"\"\n",
        "    if weight >= 1:\n",
        "        # 数字の数だけ+を追加\n",
        "        #=(E42-1)*10+1\n",
        "        num = (weight-1)*10+1\n",
        "        return s + \"+\" * int(num)\n",
        "\n",
        "    # 数字が1より小さかったら\n",
        "    else:\n",
        "        # 0.1ごとに-を追加\n",
        "        num=abs(weight-1)*10+1\n",
        "        #num = abs(weight)*10+1\n",
        "        return s + \"-\" * int(num)\n",
        "\n",
        "def get_word_weight(s):\n",
        "\n",
        "  # 正規表現パターン\n",
        "  pattern = r\":([+-]?(?:\\d+\\.?\\d*|\\.\\d+))\"\n",
        "\n",
        "  # マッチオブジェクトの取得\n",
        "  match = re.search(pattern, s)\n",
        "\n",
        "  # グループの取得\n",
        "  try:\n",
        "    result = float(match.group(1))\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    result = 1.0\n",
        "\n",
        "\n",
        "  # 文字列からグループを削除\n",
        "  s = re.sub(pattern, \"\", s)\n",
        "  return s, result\n",
        "\n",
        "def trim_parenthesis_space(string):\n",
        "    # 正規表現パターン\n",
        "    pattern = r\"( *\\()|(\\)) *\"\n",
        "\n",
        "    # 文字列を置換\n",
        "    return re.sub(pattern, \"\", string)\n",
        "\n",
        "def split_prompt(prompt):\n",
        "  # 正規表現を定義\n",
        "  pattern = r\",(?![^(]*\\))\"\n",
        "\n",
        "  # 正規表現で文字列を分割\n",
        "  result = re.split(pattern, prompt)\n",
        "\n",
        "  # 結果を表示\n",
        "  return result\n",
        "\n",
        " #########################\n",
        "  ##### control net #####\n",
        "##########################\n",
        "\n",
        "\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from controlnet_aux import HEDdetector\n",
        "from controlnet_aux import NormalBaeDetector\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from controlnet_aux import PidiNetDetector\n",
        "from diffusers.utils import load_image\n",
        "import cv2\n",
        "\n",
        "!mkdir /content/contolnet_tag/\n",
        "!mkdir /content/i2i_tag/\n",
        "\n",
        "def get_latest_file_path(folder_path):\n",
        "  \"\"\"フォルダ内の最も最近作成されたファイルのパスを取得する関数\n",
        "\n",
        "  Args:\n",
        "    folder_path: フォルダのパスを指定する\n",
        "\n",
        "  Returns:\n",
        "    フォルダ内の最も最近作成されたファイルのパスを返す\n",
        "  \"\"\"\n",
        "\n",
        "  # フォルダ内のファイル一覧を取得する\n",
        "  file_list = os.listdir(folder_path)\n",
        "\n",
        "  # ファイル一覧から、作成日時が最も新しいファイルを取得する\n",
        "  latest_file = max(file_list, key=lambda file: os.stat(os.path.join(folder_path, file)).st_ctime)\n",
        "\n",
        "  # ファイルのパスを返す\n",
        "  return os.path.join(folder_path, latest_file)\n",
        "\n",
        "def load_image_from_file(file_path):\n",
        "  \"\"\"画像ファイルをcv2に読み込む関数\n",
        "\n",
        "  Args:\n",
        "    file_path: 画像ファイルのパスを指定する\n",
        "\n",
        "  Returns:\n",
        "    cv2.imread()で読み込んだ画像データを返す\n",
        "  \"\"\"\n",
        "\n",
        "  # ファイルの拡張子を取得する\n",
        "  file_extension = os.path.splitext(file_path)[1]\n",
        "\n",
        "  # 画像ファイルの拡張子かどうかを判別する\n",
        "  if file_extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\"]:\n",
        "    # 画像ファイルをcv2に読み込む\n",
        "    return cv2.imread(file_path)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def get_control_target():\n",
        "  control_img_path = get_latest_file_path(\"/content/contolnet_tag/\")\n",
        "  img = load_image_from_file(control_img_path)\n",
        "  return img\n",
        "\n",
        "def get_i2i_target():\n",
        "  control_img_path = get_latest_file_path(\"/content/i2i_tag/\")\n",
        "  img = load_image_from_file(control_img_path)\n",
        "  return img\n",
        "\n",
        "def controlnet_canny():\n",
        "  global model_dir_path\n",
        "  tag = get_control_target()\n",
        "  low_threshold = 100\n",
        "  high_threshold = 200\n",
        "  image = cv2.Canny(tag, low_threshold, high_threshold)\n",
        "  image = image[:, :, None]\n",
        "  image = np.concatenate([image, image, image], axis=2)\n",
        "  canny_image = Image.fromarray(image)\n",
        "\n",
        "  return canny_image\n",
        "\n",
        "def resize_image(img, width, height):\n",
        "  \"\"\"\n",
        "  画像をリサイズする関数\n",
        "\n",
        "  Args:\n",
        "    input_file: 入力画像ファイル\n",
        "    output_file: 出力画像ファイル\n",
        "    width: リサイズ後の幅\n",
        "    height: リサイズ後の高さ\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  # リサイズする\n",
        "\n",
        "  img = img.resize((width, height), Image.LANCZOS)\n",
        "\n",
        "  # 画像を出力する\n",
        "  return img\n",
        "\n",
        "def controlne_normal(image):\n",
        "  depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\" )\n",
        "\n",
        "  image = depth_estimator(image)['predicted_depth'][0]\n",
        "\n",
        "  image = image.numpy()\n",
        "\n",
        "  image_depth = image.copy()\n",
        "  image_depth -= np.min(image_depth)\n",
        "  image_depth /= np.max(image_depth)\n",
        "\n",
        "  bg_threhold = 0.4\n",
        "\n",
        "  x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "  x[image_depth < bg_threhold] = 0\n",
        "\n",
        "  y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "  y[image_depth < bg_threhold] = 0\n",
        "\n",
        "  z = np.ones_like(x) * np.pi * 2.0\n",
        "\n",
        "  image = np.stack([x, y, z], axis=2)\n",
        "  image /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
        "  image = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
        "  image = Image.fromarray(image)\n",
        "  return image\n",
        "\n",
        "  ########################\n",
        "  ###### load rola ######\n",
        "  ########################\n",
        "\n",
        "\n",
        "# 1,loraフォルダからぜｎLora を読み込んでおく\n",
        "# prompt からloraをひろってもとから消す\n",
        "# filenameとウェイトを読んでfuse_loraに渡す\n",
        "import re\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "class lora_loader:\n",
        "  lora_dir = \"\"\n",
        "  pipe = None\n",
        "  LORA_DICT = {}\n",
        "  def __init__(self, pipe, fuse_loras):\n",
        "    self.lora_dir = \"/content/drive/MyDrive/models/Lora/\" # @param {type:\"string\"}\n",
        "    self.pipe = pipe\n",
        "    self.fuse_loras = fuse_loras\n",
        "    self.LORA_DICT = self.generate_filename_path_dict(self.find_files_as_dict(self.lora_dir, \"safetensors\"))\n",
        "    print(fuse_loras)\n",
        "\n",
        "  def find_files_as_dict(self,dir, ext):\n",
        "    p = Path(dir)\n",
        "    l = list(p.glob(f\"**/*.{ext}\"))\n",
        "    return l\n",
        "\n",
        "  def generate_filename_path_dict(self,filepaths):\n",
        "    filename_path_dict = {}\n",
        "    for filepath in filepaths:\n",
        "      filename = os.path.splitext(os.path.basename(filepath))[0]\n",
        "      filename_path_dict[filename] = filepath\n",
        "    print(f\"{len(filename_path_dict)} loras found.\")\n",
        "    return filename_path_dict\n",
        "\n",
        "  def get_lora(self, name):\n",
        "    try:\n",
        "      return self.LORA_DICT[name]\n",
        "    except Exception as e:\n",
        "      return None\n",
        "\n",
        "  def set_loras(self):\n",
        "    name_list = []\n",
        "    scale_list = []\n",
        "    formed_fuse_loras = self.merge_values(self.fuse_loras)\n",
        "    self.pipe.set_adapters(name_list,adapter_weights=scale_list)\n",
        "    for lora in formed_fuse_loras:\n",
        "      lora_name = lora[0]\n",
        "      lora_path = self.get_lora(lora_name)\n",
        "      scale = lora[1]\n",
        "      if type(scale) is not float:\n",
        "        scale = float(1)\n",
        "        print(f\"{lora_name}'s scale is not float. set 1.\")\n",
        "      if lora_path == None:\n",
        "        print(f\"{lora_name} is None. pass\")\n",
        "        write_log(lora_name+\" is not exists.\")\n",
        "        continue\n",
        "      elif os.path.exists(lora_path):\n",
        "        print(f\"loading{lora_path}\")\n",
        "      lora_dir = os.path.dirname(lora_path)\n",
        "      lora_file = os.path.basename(lora_path)\n",
        "      adapt_name = lora_name.replace(\".\",\"_\")\n",
        "      try:\n",
        "        self.pipe.load_lora_weights(lora_dir, weight_name=lora_file, lora_scale=scale, low_cpu_mem_usage=False, adapter_name=adapt_name)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        print(f\"error occured in {lora_name}, continue\")\n",
        "        write_log(lora_name + \"\" + str(e))\n",
        "        continue\n",
        "      name_list.append(adapt_name)\n",
        "      scale_list.append(scale)\n",
        "    self.pipe.set_adapters(name_list,adapter_weights=scale_list)\n",
        "    self.pipe.fuse_lora()\n",
        "    self.pipe.unload_lora_weights()\n",
        "    #self.pipe.unet.to(memory_format=torch.channels_last)\n",
        "    #self.pipe.unet = torch.compile(self.pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "  def load_lora(self,lora_name, scale):\n",
        "    lora_path = self.get_lora(lora_name)\n",
        "    if type(scale) is not float:\n",
        "      scale = float(1)\n",
        "      print(f\"{lora_name}'s scale is not float. set 1.\")\n",
        "    if lora_path == None:\n",
        "      print(f\"{lora_name} is None.\")\n",
        "      write_log(lora_name+\" is not exists.\")\n",
        "    elif os.path.exists(lora_path):\n",
        "      print(f\"loading{lora_path}\")\n",
        "      try:\n",
        "        #self.pipe.load_lora_weights(lora_path)\n",
        "        #dir_path, file_nameをあたえる\n",
        "        lora_dir = os.path.dirname(lora_path)\n",
        "        lora_file = os.path.basename(lora_path)\n",
        "        self.pipe.load_lora_weights(lora_dir, weight_name=lora_file, lora_scale=scale, low_cpu_mem_usage=False)\n",
        "        #self.pipe.load_lora_weights(\".\", weight_name=lora_path, lora_scale =scale)\n",
        "        # load_safetensors_lora(self.pipe, lora_path) layer syuturyoku nomi\n",
        "        self.pipe.fuse_lora(lora_scale=scale)\n",
        "        print(f\"{lora_name} fused.\")\n",
        "      except Exception as e:\n",
        "        print(f\"error in loading {lora_name}, may be not lora.\")\n",
        "        print(e)\n",
        "    else:\n",
        "       print(f\"probably {lora_name} is not exists.\")\n",
        "\n",
        "  def fusing_loras(self):\n",
        "    self.set_loras()\n",
        "    #2024-11-13 Loraの発言不全にともなってLora適応方法をfuseからadaptにかえる\n",
        "    #for lora in self.fuse_loras:\n",
        "    #  self.load_lora(lora[0], lora[1])\n",
        "\n",
        "  def merge_values(self,a_list):\n",
        "      result = {}  # 結果を格納するための空の辞書\n",
        "      for item in a_list:\n",
        "          key = item[0]    # 0番目の要素をキーに\n",
        "          value = item[1]  # 1番目の要素を値として取得\n",
        "          if key in result:\n",
        "              result[key] += value  # 既にキーが存在すれば値を足し合わせる\n",
        "          else:\n",
        "              result[key] = value  # キーが存在しなければ新しく追加\n",
        "      # 結果をリストに変換\n",
        "      return [[key, value] for key, value in result.items()]\n",
        "\n",
        "\n",
        "def get_loras_from_prompt(prompt):\n",
        "  \"\"\"\n",
        "  文字列から<lora:[^>]+>に該当する文字列を抽出する\n",
        "\n",
        "  Args:\n",
        "    text: 抽出対象の文字列\n",
        "\n",
        "  Returns:\n",
        "    抽出された文字列\n",
        "  \"\"\"\n",
        "\n",
        "  pattern = re.compile(r\"<lora:[^>]+>\")\n",
        "  match = pattern.findall(prompt)\n",
        "  print(\"get loras from prompt\")\n",
        "  result = []\n",
        "  for mat in match:\n",
        "    try:\n",
        "      splitted = mat.replace(\"<\",\"\")\n",
        "      splitted = splitted.replace(\">\",\"\")\n",
        "      splitted = splitted.split(\":\")\n",
        "      if len(splitted) < 3:\n",
        "        print(f\"{mat}, failed to split.\")\n",
        "      else:\n",
        "        print(f\"{splitted}, \")\n",
        "        name = splitted[1]\n",
        "        weight = float(splitted[2])\n",
        "        result.append([name,weight])\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "  return result\n",
        "\n",
        "\n",
        "def remove_lora(text):\n",
        "  removed_prompt = re.sub(r\"<lora:[^>]+>\", \"\", text)\n",
        "  print(\"removed prompt:\")\n",
        "  print(removed_prompt)\n",
        "  return removed_prompt\n",
        "\n",
        "\n",
        "# load_textual_inversion\n",
        "\n",
        "def set_embs(prompt, pipe):\n",
        "  emb_paths = fined_emb_from_dir(\"/content/drive/MyDrive/models/textual_inversion\")\n",
        "  emb_dict = make_emb_dict(emb_paths)\n",
        "  splitted_prompt = split_and_strip_list_comprehension(prompt)\n",
        "  for word in splitted_prompt:\n",
        "    if word in emb_dict:\n",
        "      # 存在する場合は、キーに対応する値を返す\n",
        "      if os.path.exists(emb_dict[word]):\n",
        "        pipe.load_textual_inversion(emb_dict[word], token=word)\n",
        "        print(f\"{word} is texture. loaded.\")\n",
        "\n",
        "def fined_emb_from_dir(dir):\n",
        "  p = Path(dir)\n",
        "  l = list(p.glob(f\"**/*.pt\"))\n",
        "  return l\n",
        "\n",
        "def make_emb_dict(filepaths):\n",
        "  filename_path_dict = {}\n",
        "  for filepath in filepaths:\n",
        "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    filename_path_dict[filename] = filepath\n",
        "  print(f\"{len(filename_path_dict)} embds found.\")\n",
        "  return filename_path_dict\n",
        "\n",
        "def split_and_strip_list_comprehension(string):\n",
        "\n",
        "  # 文字列を\",\"で区切って、各文字列から前後のスペースを取り除く\n",
        "  return [s.strip() for s in string.split(\",\")]\n",
        "\n",
        "def load_safetensors_lora(pipeline, checkpoint_path, LORA_PREFIX_UNET=\"lora_unet\", LORA_PREFIX_TEXT_ENCODER=\"lora_te\", alpha=0.75):\n",
        "    # load LoRA weight from .safetensors\n",
        "    state_dict = load_file(checkpoint_path)\n",
        "\n",
        "    visited = []\n",
        "    print(f\"checkpoint_path: {checkpoint_path}, lora test\")\n",
        "    # directly update weight in diffusers model\n",
        "    for key in state_dict:\n",
        "        print(f\"lora key: {key}\")\n",
        "        # it is suggested to print out the key, it usually will be something like below\n",
        "        # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "        continue\n",
        "        # as we have set the alpha beforehand, so just skip\n",
        "        if \".alpha\" in key or key in visited:\n",
        "            continue\n",
        "\n",
        "        if \"text\" in key:\n",
        "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.text_encoder\n",
        "        else:\n",
        "            layer_infos = key.split(\".\")[0].split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "            curr_layer = pipeline.unet\n",
        "\n",
        "        # find the target layer\n",
        "        temp_name = layer_infos.pop(0)\n",
        "        while len(layer_infos) > -1:\n",
        "            try:\n",
        "                curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                if len(layer_infos) > 0:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "                elif len(layer_infos) == 0:\n",
        "                    break\n",
        "            except Exception:\n",
        "                if len(temp_name) > 0:\n",
        "                    temp_name += \"_\" + layer_infos.pop(0)\n",
        "                else:\n",
        "                    temp_name = layer_infos.pop(0)\n",
        "\n",
        "        pair_keys = []\n",
        "        if \"lora_down\" in key:\n",
        "            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n",
        "            pair_keys.append(key)\n",
        "        else:\n",
        "            pair_keys.append(key)\n",
        "            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n",
        "\n",
        "        # update weight\n",
        "        if len(state_dict[pair_keys[0]].shape) == 4:\n",
        "            weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "            weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n",
        "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n",
        "        else:\n",
        "            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n",
        "            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n",
        "            curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n",
        "\n",
        "        # update visited list\n",
        "        for item in pair_keys:\n",
        "            visited.append(item)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def write_log(message, filename=\"log.txt\", mode=\"a\"):\n",
        "    with open(filename, mode) as f:\n",
        "        f.write(f\"{message}\\n\")\n",
        "\n",
        "########################\n",
        "###### meta data ######\n",
        "########################\n",
        "\n",
        "# @title metadata\n",
        "from PIL import PngImagePlugin\n",
        "\n",
        "def add_metadata(img, metadata: dict):\n",
        "    imgInfo = img.info\n",
        "    imgInfo.update(metadata)\n",
        "    info = PngImagePlugin.PngInfo()\n",
        "    for k, v in imgInfo.items():\n",
        "      print(k,v)\n",
        "      info.add_itxt(k, str(v))\n",
        "    return info\n",
        "\n",
        "\n",
        "\n",
        "def init_metadata(input_path: str, output_path: str, metadata: dict):\n",
        "    img = PngImagePlugin.PngImageFile(input_path)\n",
        "    info = PngImagePlugin.PngInfo()\n",
        "    for k, v in metadata.items():\n",
        "        info.add_itxt(k, v)\n",
        "    img.save(output_path, pnginfo=info)\n",
        "\n",
        "\n",
        "def get_metadata(path: str):\n",
        "    img = PngImagePlugin.PngImageFile(path)\n",
        "    info = img.info\n",
        "    for key, value in info.items():\n",
        "        print(f'{key}: {value}')\n",
        "    return info\n",
        "\n",
        "def save_meta(img, save_path, prompt=\"\", nega=\"\", step=\"\", Sampler=\"\", CFG_scale=\"\", seed=\"\", size=\"\", Model_hash=\"\", Model=\"\", strength=\"\", Mask_blur=\"\"):\n",
        "    metadata = {}\n",
        "    metadata[\"parameters\"] = prompt\n",
        "    metadata[\"Negative prompt\"] = nega\n",
        "\n",
        "    metadata[\"Step\"] = step\n",
        "    metadata[\"Sampler\"] = Sampler\n",
        "    metadata[\"CFG scale\"] = CFG_scale\n",
        "    metadata[\"Seed\"] = seed\n",
        "    metadata[\"Size\"] = size\n",
        "    metadata[\"Model hash\"] = Model_hash\n",
        "    metadata[\"Model\"] = Model\n",
        "    metadata[\"Denoising strength\"] = strength\n",
        "    metadata[\"Mask blur\"] = Mask_blur\n",
        "\n",
        "    info = add_metadata(img, metadata)\n",
        "    img.save(save_path, pnginfo=info)\n",
        "\n",
        "###########################\n",
        "###### image adjust ######\n",
        "###########################\n",
        "\n",
        "# @title Image Adjust\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def image_adjust(img, gamma=2):\n",
        "  img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  # BGRからHSVに変換\n",
        "  h, s, v = cv2.split(img_hsv)  # チャンネルごとに分割\n",
        "\n",
        "  # ルックアップテーブルの生成\n",
        "  look_up_table = np.zeros((256,1),dtype=np.uint8)\n",
        "  for i in range(256):\n",
        "      look_up_table[i][0] = (i/255)**(1.0/gamma)*255\n",
        "\n",
        "  s_lut = cv2.LUT(s, look_up_table)  # 彩度(S)に対してルックアップテーブル適用\n",
        "  img_merge = cv2.merge([h, s_lut, v])  #  H,変換後S,Vをマージ\n",
        "  img_bgr = cv2.cvtColor(img_merge, cv2.COLOR_HSV2BGR)  # HSVからBGRに変換\n",
        "\n",
        "  return img_bgr\n",
        "\n",
        "def manual_adjust(img, sat, con):\n",
        "  if sat > 0:\n",
        "    base_image = add_saturation(img, sat)\n",
        "  if con > 0:\n",
        "    base_image = add_contrast(img, con)\n",
        "\n",
        "  return base_image\n",
        "\n",
        "def adjust_contract(img, factor=1):\n",
        "  img = img.convert(\"YCbCr\")\n",
        "  yy, cb, cr = img.split()\n",
        "\n",
        "  yy = ImageOps.equalize(yy)\n",
        "  yy * factor\n",
        "  img = Image.merge(\"YCbCr\", (yy, cb, cr))\n",
        "  img = img.convert(\"RGB\")\n",
        "  return img\n",
        "\n",
        "def get_saturation(img):\n",
        "    a\n",
        "    # 画像の幅と高さを取得\n",
        "    width, height = img.size\n",
        "\n",
        "    # 画像のピクセルデータを取得\n",
        "    pixels = img.load()\n",
        "\n",
        "    # 各ピクセルの彩度をリストに格納\n",
        "    saturation_list = []\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            # RGB値を取得\n",
        "            r, g, b = pixels[x, y]\n",
        "\n",
        "            # 彩度を計算\n",
        "            saturation = (max(r, g, b) - min(r, g, b)) / 3\n",
        "\n",
        "            # 彩度リストに追加\n",
        "            saturation_list.append(saturation)\n",
        "            average_saturation = sum(saturation_list) / len(saturation_list)\n",
        "\n",
        "            # 結果を出力\n",
        "            return average_saturation\n",
        "\n",
        "def adjust_saturation(img, tag_sat, current_sat):\n",
        "    enhancer = ImageEnhance.Color(img)\n",
        "    saturation_factor = tag_sat / current_sat\n",
        "    print(f\"saturation_factor: {str(saturation_factor)}\")\n",
        "    enhanced_image = enhancer.enhance(saturation_factor)\n",
        "    return enhanced_image\n",
        "\n",
        "def    auto_enhance_image(image):\n",
        "    \"\"\"\n",
        "    Pillowを使って彩度やコントラストを自動で調整し、鮮明にする処理\n",
        "\n",
        "    Args:\n",
        "        image_path:    画像ファイルパス\n",
        "\n",
        "    Returns:\n",
        "        鮮明化された画像\n",
        "    \"\"\"\n",
        "\n",
        "    # 画像を開く\n",
        "\n",
        "    # RGBチャンネルを取得\n",
        "    r, g, b = image.split()\n",
        "\n",
        "    #    ヒストグラム分析\n",
        "    hist_r = np.histogram(r, 256)[0]\n",
        "    hist_g = np.histogram(g, 256)[0]\n",
        "    hist_b = np.histogram(b, 256)[0]\n",
        "\n",
        "    # Otsu's methodで閾値を計算\n",
        "    threshold_r = OtsuThreshold(hist_r)\n",
        "    threshold_g = OtsuThreshold(hist_g)\n",
        "    threshold_b = OtsuThreshold(hist_b)\n",
        "\n",
        "    # 閾値に基づいて彩度とコントラストを調整\n",
        "    enhancer_r = ImageEnhance.Contrast(r)\n",
        "    enhancer_g = ImageEnhance.Contrast(g)\n",
        "    enhancer_b = ImageEnhance.Contrast(b)\n",
        "\n",
        "    factor_r = 1.0 + (1.2 - 1.0) / (255 - threshold_r)\n",
        "    factor_g = 1.0 + (1.2 - 1.0) / (255 - threshold_g)\n",
        "    factor_b = 1.0 + (1.2 - 1.0) / (255 - threshold_b)\n",
        "\n",
        "    r = enhancer_r.enhance(factor_r)\n",
        "    g = enhancer_g.enhance(factor_g)\n",
        "    b = enhancer_b.enhance(factor_b)\n",
        "\n",
        "    # 合成して保存\n",
        "    image = Image.merge(\"RGB\", (r, g, b))\n",
        "\n",
        "    return image\n",
        "\n",
        "#    Otsu's method\n",
        "def OtsuThreshold(hist):\n",
        "    total = hist.sum()\n",
        "    sumB = 0\n",
        "    wB = 0\n",
        "    varMax = 0\n",
        "    threshold = 0\n",
        "\n",
        "    for i in range(0, 256):\n",
        "        wB += hist[i]\n",
        "        if wB == 0:\n",
        "            continue\n",
        "\n",
        "        wF = total - wB\n",
        "        sumB += i * hist[i]\n",
        "\n",
        "        mB = sumB / wB\n",
        "        mF = (total * sumB - sumB) / wF\n",
        "\n",
        "        varBetween = wB * wF * (mB - mF) ** 2\n",
        "\n",
        "        if varBetween >= varMax:\n",
        "            threshold = i\n",
        "            varMax = varBetween\n",
        "\n",
        "    return threshold\n",
        "\n",
        "def add_saturation(img, factor):\n",
        "  enhancer = ImageEnhance.Color(img)  # ImageEnhance.Colorオブジェクト生成\n",
        "\n",
        "  factor = [factor,factor,factor]  # enhancement factorのリスト\n",
        "\n",
        "  for i in factor:\n",
        "      image_c = enhancer.enhance(i)\n",
        "\n",
        "  return image_c\n",
        "\n",
        "def add_contrast(img, factor):\n",
        "  image = ImageEnhance.Contrast(img)\n",
        "  image = image.enhance(factor)\n",
        "  return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2X2HbEtM6R9"
      },
      "source": [
        "# GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d1Ab4Mu-NF57"
      },
      "outputs": [],
      "source": [
        "# @title Generate Random_portrait generator\n",
        "\n",
        "#test prompt\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "prompt = \"\" # @param {type:\"string\"}\n",
        "add_prompt = \"\" # @param {type:\"string\"}\n",
        "prompt = prompt + \", \" + add_prompt\n",
        "neg = \"\" # @param {type:\"string\"}\n",
        "add_neg = \"\" # @param {type:\"string\"}\n",
        "neg = add_neg + \",\" + neg\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Randomizer\n",
        "is_random_hair = False # @param {type:\"boolean\"}\n",
        "is_random_face = False # @param {type:\"boolean\"}\n",
        "gender = \"none\" # @param [\"none\",\"female\",\"young_female\",\"male\",\"young_male\",\"anime_girl\",\"anime_boy\"]\n",
        "fix_hair_color = \"\" # @param {type:\"string\"}\n",
        "fix_eyes_color = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "sampling_steps = 30 # @param {type:\"integer\"}\n",
        "CFG_Scale = 7 # @param {type:\"number\"}\n",
        "Denoising_strength = 0.5 # @param {type: \"number\"}\n",
        "\n",
        "height = 900 # @param {type:\"integer\"}\n",
        "width = 673 # @param {type:\"integer\"}\n",
        "count = 3 # @param {type:\"integer\"}\n",
        "seed = -1 # @param {type:\"integer\"}\n",
        "resize_factor = 1.5 # @param {type:\"number\"}\n",
        "sampler = \"EulerAncestralDiscrete\" # @param ['DDIM', 'DDPM', 'DEISMultistep', 'DPMSolverMultistep', 'DPMSolverSinglestep', 'EulerAncestralDiscrete', 'EulerDiscrete', 'HeunDiscrete', 'KDPM2AncestralDiscrete', 'KDPM2Discrete', 'UniPCMultistep']\n",
        "resize_step_factor = 1.5 # @param {type:\"number\"}\n",
        "resize_mode = \"latent\" # @param [\"latent\", \"2x\"]\n",
        "hires_strength = 0.1 # @param {type:\"number\"}\n",
        "clip_skip = 2 # @param {type:\"integer\"}\n",
        "controlnet_type = \"sketch\" # @param [\"none\", \"sketch\", \"canny\", \"softedge\",\"pix2pix\",\"bae\",\"pose\",\"i2i\"]\n",
        "controlnet_factor = 0.4 # @param {type:\"number\"}\n",
        "controlnet_factor = float(controlnet_factor)\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device)\n",
        "\n",
        "!mkdir /content/save_dir/\n",
        "!mkdir /content/saved_all/\n",
        "\n",
        "width = int(width // 64) * 64\n",
        "height = int(height // 64) * 64\n",
        "\n",
        "def main():\n",
        "  global seed,resize_factor, sampling_steps,height, width, sampler,CFG_Scale,Denoising_strength,prompt,gender,fix_hair_color,fix_eyes_color,is_random_hair,is_random_face\n",
        "  device = \"cuda\"\n",
        "  with torch.inference_mode():\n",
        "    save_dir = \"/content/save_dir/\"\n",
        "    local_seed = get_seed(seed)\n",
        "    delete_all_files(save_dir)\n",
        "    if controlnet_type != \"none\":\n",
        "      print(\"controlnet_type is not none\")\n",
        "      if controlnet_type == \"canny\":\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
        "        con_img = controlnet_canny()\n",
        "      elif controlnet_type == \"sketch\":\n",
        "        hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "        con_img = hed(get_control_target(), scribble=True)\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_scribble\", torch_dtype=torch.float16)\n",
        "      elif controlnet_type == \"softedge\":\n",
        "        #con_img = controlnet_normal(get_control_target())\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_softedge\", torch_dtype=torch.float16)\n",
        "        processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "        processor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\n",
        "        con_img = processor(get_control_target(), safe=True)\n",
        "      elif controlnet_type == \"pix2pix\":\n",
        "        con_img = get_control_target()\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11e_sd15_ip2p\", torch_dtype=torch.float16)\n",
        "      elif controlnet_type == \"bae\":\n",
        "        detecter = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "        con_img = detecter(get_control_target())\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_normalbae\", torch_dtype=torch.float16)\n",
        "      elif controlnet_type == \"pose\":\n",
        "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_openpose\", torch_dtype=torch.float16)\n",
        "        posedetecter = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
        "        con_img = posedetecter(get_control_target(), hand_and_face=True)\n",
        "      elif controlnet_type == \"i2i\":\n",
        "        con_img = Image.open(get_control_target())\n",
        "\n",
        "      controlnet.mid_block_scale_factor = controlnet_factor\n",
        "      con_img = resize_image(con_img, width, height)\n",
        "      pipe = StableDiffusionControlNetPipeline.from_single_file(\n",
        "          model_path,\n",
        "          controlnet=controlnet,\n",
        "          torch_dtype=torch.float16,\n",
        "          vae=vae,\n",
        "          load_safety_checker=False,\n",
        "          extract_ema=True,\n",
        "          custom_pipeline=\"lpw_stable_diffusion\",\n",
        "          clip_skip=2,\n",
        "          strength=Denoising_strength\n",
        "      )\n",
        "      #print(str(**pipe.components))\n",
        "      i2i_pipe = StableDiffusionImg2ImgPipeline.from_single_file(\n",
        "          model_path,\n",
        "          torch_dtype=torch.float16,\n",
        "          vae=vae,\n",
        "          load_safety_checker=False,\n",
        "          extract_ema=True,\n",
        "          custom_pipeline=\"lpw_stable_diffusion\",\n",
        "          clip_skip=2,\n",
        "          strength=hires_strength,\n",
        "      )\n",
        "    else:\n",
        "      print(\"no controlnet\")\n",
        "      pipe = StableDiffusionPipeline.from_single_file(\n",
        "          model_path,\n",
        "          torch_dtype=torch.float16,\n",
        "          use_safetensors=True,\n",
        "          vae=vae,\n",
        "          load_safety_checker=False,\n",
        "          extract_ema=True,\n",
        "          custom_pipeline=\"lpw_stable_diffusion\",\n",
        "          clip_skip=2,\n",
        "          strength=Denoising_strength\n",
        "          )\n",
        "      i2i_pipe = StableDiffusionImg2ImgPipeline(**pipe.components)\n",
        "\n",
        "    pipe.scheduler = SCHEDULERS_LIST[sampler].from_pretrained(\n",
        "        pretrained_model_name_or_path=\"mfrashad/hassakuv13\",\n",
        "        torch_dtype=torch.float16,\n",
        "        cache_dir=\"./cache\",\n",
        "        subfolder='scheduler'\n",
        "    )\n",
        "\n",
        "\n",
        "    i2i_pipe.scheduler = SCHEDULERS_LIST[sampler].from_pretrained(\n",
        "        pretrained_model_name_or_path=\"mfrashad/hassakuv13\",\n",
        "        torch_dtype=torch.float16,\n",
        "        cache_dir=\"./cache\",\n",
        "        subfolder='scheduler',\n",
        "    )\n",
        "    try:\n",
        "      pipe = pipe.to(\"cuda\")\n",
        "      i2i_pipe.to(\"cuda\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    print(\"ram reduce.\")\n",
        "    # ram reduce\n",
        "\n",
        "    #pipe.enable_model_cpu_offload()\n",
        "    #i2i_pipe.enable_model_cpu_offload()\n",
        "\n",
        "    pipe.enable_vae_tiling()\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "    #i2i_pipe.enable_vae_tiling()\n",
        "    i2i_pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "\n",
        "    # random json reader\n",
        "    # @markdown ----\n",
        "    rnd_dir_path = \"/content/drive/MyDrive/models/random_list/\" # @param {type:\"string\"}\n",
        "    rnd_files = get_rnd_json_files(rnd_dir_path)\n",
        "    print(rnd_files)\n",
        "\n",
        "    rnd_data = merge_json(rnd_files)\n",
        "    print(f\"Random Face: {str(is_random_face)}, Random Hair: {str(is_random_hair)}\")\n",
        "    if is_random_hair == True:\n",
        "        if fix_hair_color == \"\":\n",
        "            fix_eyes_color = None\n",
        "        prompt += make_randomized_hair(rnd_data,fix_hair_color)\n",
        "\n",
        "    if is_random_face == True:\n",
        "        if fix_eyes_color == \"\":\n",
        "            fix_eyes_color = None\n",
        "        prompt += make_randomized_face(rnd_data,fix_eyes_color,gender)\n",
        "\n",
        "    if gender != \"none\":\n",
        "        add_chara = random.choice(rnd_data[gender])\n",
        "        prompt += f\"({add_chara}),\"\n",
        "\n",
        "    # prompt tokenize\n",
        "    lora_list = get_loras_from_prompt(prompt)\n",
        "    loader = lora_loader(pipe, lora_list)\n",
        "    loader.fusing_loras()\n",
        "    non_lora_prompt = remove_lora(prompt)\n",
        "    non_lora_neg = remove_lora(neg)\n",
        "    set_embs(non_lora_neg, pipe)\n",
        "\n",
        "    positive_embeds, negative_embeds = get_prompt_with_weight(pipe, non_lora_prompt,non_lora_neg)\n",
        "    #positive_embeds, negative_embeds = token_auto_concat_embeds(pipe, non_lora_prompt,non_lora_neg)\n",
        "\n",
        "    #textual_inversion_manager = DiffusersTextualInversionManager(pipe)\n",
        "    #compel = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder,\n",
        "    #textual_inversion_manager=textual_inversion_manager,\n",
        "    #    truncate_long_prompts=False,\n",
        "    #    device=\"cuda\")\n",
        "\n",
        "    #positive_embeds = compel(tokenize_line(non_lora_prompt, pipe.tokenizer))\n",
        "    #negative_embeds = compel(tokenize_line(non_lora_neg, pipe.tokenizer))\n",
        "    #positive_embeds, negative_embeds = compel.pad_conditioning_tensors_to_same_length([concat_tensor(positive_embeds), concat_tensor(negative_embeds)])\n",
        "    ##################\n",
        "    #pipe.enable_xformers_memory_efficient_attention()\n",
        "    #pipe.enable_model_cpu_offload()\n",
        "    #i2i_pipe.enable_xformers_memory_efficient_attention()\n",
        "    #i2i_pipe.enable_model_cpu_offload()\n",
        "    start = time.time()\n",
        "    for i in range(0, count, 1):\n",
        "      local_seed = local_seed + i\n",
        "      if controlnet_type != \"none\":\n",
        "        base_image = gen_control_base_img(pipe, local_seed, positive_embeds, negative_embeds, con_img, lora_list)\n",
        "      elif controlnet_type == \"i2i\":\n",
        "        base_image = con_img\n",
        "      else:\n",
        "        base_image = gen_base_img(pipe, local_seed, positive_embeds, negative_embeds,lora_list)\n",
        "      if resize_mode == \"2x\":\n",
        "        width, height = base_image.size\n",
        "        new_width = int(width * resize_factor)\n",
        "        new_height = int(height * resize_factor)\n",
        "        base_image = gen_upscaler_by_diffusers(base_image, local_seed)\n",
        "        base_image = upscale_image2(base_image, new_width, new_height)\n",
        "        upscaled_img = gen_upscale_img(i2i_pipe, base_image, 1,local_seed,positive_embeds, negative_embeds)\n",
        "      else:\n",
        "        #loader2 = lora_loader(i2i_pipe, lora_list)\n",
        "        #loader2.fusing_loras()\n",
        "        upscaled_img = gen_upscale_img(i2i_pipe, base_image, resize_factor,local_seed,positive_embeds, negative_embeds)\n",
        "      #dif_upscaled_img = gen_upscaler_by_diffusers(base_image,local_seed)\n",
        "      base_name = str(int(time.time())) + \"_\" + str(local_seed)\n",
        "      filename = os.path.join(save_dir,base_name + \".png\")\n",
        "      base_flename = os.path.join(save_dir,base_name + \"_base.png\")\n",
        "\n",
        "      allsave_name = os.path.join(\"/content/saved_all/\", base_name + \".png\")\n",
        "      if controlnet_type != \"none\":\n",
        "        con_img.save(\"con_tag.png\")\n",
        "      #diffup_filename = os.path.join(save_dir, base_name + \"_difscaled.png\")\n",
        "\n",
        "      #meta save\n",
        "      meta_size = f\"{str(width)}x{str(height)}\"\n",
        "      meta_model_name = os.path.basename(model_path)\n",
        "      save_meta(upscaled_img, filename,prompt, neg, sampling_steps,\n",
        "                               sampler, CFG_Scale, local_seed, meta_size, \"None\", meta_model_name,\n",
        "                               Denoising_strength, 0)\n",
        "\n",
        "      #upscaled_img.save(filename)\n",
        "      #base_image.save(base_flename)\n",
        "\n",
        "      upscaled_img.save(allsave_name)\n",
        "      #dif_upscaled_img.save(diffup_filename)\n",
        "    end = time.time()\n",
        "    print(f\"time elapsed: {end-start}sec\")\n",
        "    #end\n",
        "\n",
        "\n",
        "def get_seed(seed):\n",
        "  if seed < 0:\n",
        "    return random.randrange(0, 4294967295, 1)\n",
        "  else:\n",
        "    return seed\n",
        "\n",
        "def gen_base_img(pipe,seed, positive_embeds, negative_embeds, lora_list):\n",
        "  global width, height, CFG_Scale, prompt, neg, sampling_steps,clip_skip, generator\n",
        "  gen_seed = get_seed(seed)\n",
        "  generator.manual_seed(gen_seed)\n",
        "  result = pipe(\n",
        "      prompt_embeds=positive_embeds,\n",
        "      negative_prompt_embeds=negative_embeds,\n",
        "      guidance_scale=CFG_Scale,\n",
        "      width=width,\n",
        "      height=height,\n",
        "      generator=generator,\n",
        "      num_inference_steps=sampling_steps,\n",
        "      output_type=\"latent\"\n",
        "    ).images\n",
        "  with torch.no_grad():\n",
        "      image = pipe.decode_latents(result)\n",
        "  image = pipe.numpy_to_pil(image)[0]\n",
        "  return image\n",
        "\n",
        "def gen_base_img_i2i(pipe,seed, positive_embeds, negative_embeds, lora_list,i2i_image):\n",
        "  global width, height, CFG_Scale, prompt, neg, sampling_steps,clip_skip, generator\n",
        "  gen_seed = get_seed(seed)\n",
        "  generator.manual_seed(gen_seed)\n",
        "  result = pipe(\n",
        "      prompt_embeds=positive_embeds,\n",
        "      negative_prompt_embeds=negative_embeds,\n",
        "      guidance_scale=CFG_Scale,\n",
        "      width=width,\n",
        "      height=height,\n",
        "      generator=generator,\n",
        "      num_inference_steps=sampling_steps,\n",
        "      output_type=\"latent\",\n",
        "      image = i2i_image\n",
        "    ).images\n",
        "  with torch.no_grad():\n",
        "      image = pipe.decode_latents(result)\n",
        "  image = pipe.numpy_to_pil(image)[0]\n",
        "  return image\n",
        "\n",
        "def gen_control_base_img(pipe,seed, positive_embeds, negative_embeds, con_img, lora_list):\n",
        "  global width, height, CFG_Scale, prompt, neg, sampling_steps,clip_skip, generator\n",
        "  gen_seed = get_seed(seed)\n",
        "  generator.manual_seed(gen_seed)\n",
        "  result = pipe(\n",
        "      prompt_embeds=positive_embeds,\n",
        "      negative_prompt_embeds=negative_embeds,\n",
        "      image=con_img,\n",
        "      guidance_scale=CFG_Scale,\n",
        "      width=width,\n",
        "      height=height,\n",
        "      generator=generator,\n",
        "      num_inference_steps=sampling_steps,\n",
        "      output_type=\"latent\"\n",
        "    ).images\n",
        "  with torch.no_grad():\n",
        "      image = pipe.decode_latents(result)\n",
        "  image = pipe.numpy_to_pil(image)[0]\n",
        "  return image\n",
        "\n",
        "def gen_upscale_img(i2i_pipe, img,factor,seed,positive_embeds,negative_embeds):\n",
        "  global width, height, CFG_Scale, prompt, neg, sampling_steps,clip_skip, generator,resize_step_factor,hires_strength\n",
        "  resized_image = upscale_image(img, factor)\n",
        "  resized_image.save(\"resized.png\")\n",
        "  i2i_step = int(sampling_steps*resize_step_factor)\n",
        "  img2img = i2i_pipe(\n",
        "      prompt_embeds=positive_embeds,\n",
        "      negative_prompt_embeds=negative_embeds,\n",
        "      image=resized_image,\n",
        "      num_inference_steps=i2i_step,\n",
        "      output_type=\"latent\",\n",
        "      strength=hires_strength,\n",
        "    ).images\n",
        "  with torch.no_grad():\n",
        "      image = i2i_pipe.decode_latents(img2img)\n",
        "  image = i2i_pipe.numpy_to_pil(image)[0]\n",
        "  return image\n",
        "\n",
        "def gen_upscaler_by_diffusers(img, seed):\n",
        "  upscale_model_id = \"stabilityai/sd-x2-latent-upscaler\"\n",
        "  upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(upscale_model_id, torch_dtype=torch.float16)\n",
        "  upscaler.to(\"cuda\")\n",
        "\n",
        "  upscaled_image = upscaler(\n",
        "      prompt=prompt,\n",
        "      image=img,\n",
        "      num_inference_steps=20,\n",
        "      guidance_scale=0,\n",
        "      generator=generator,\n",
        "  ).images[0]\n",
        "  return upscaled_image\n",
        "\n",
        "def upscale_image(image, factor, resample=Image.LANCZOS):\n",
        "    \"\"\"\n",
        "    画像を何倍にするかの引数を受け取って、imageのsizeから新しい画像のサイズを計算して小数点を丸めた値でリサイズする\n",
        "\n",
        "    Args:\n",
        "        image: リサイズ対象の画像\n",
        "        factor: リサイズ倍率\n",
        "        resample: リサンプリング方法\n",
        "\n",
        "    Returns:\n",
        "        リサイズ後の画像\n",
        "    \"\"\"\n",
        "\n",
        "    width, height = image.size\n",
        "\n",
        "    # 新しい画像のサイズを計算\n",
        "    new_width = int(width * factor)\n",
        "    new_height = int(height * factor)\n",
        "\n",
        "    # 小数点を丸めてリサイズ\n",
        "    image = image.resize((new_width, new_height), resample)\n",
        "    enhancer = ImageEnhance.Sharpness(image)\n",
        "    # enhancerオブジェクトの強調\n",
        "    image = enhancer.enhance(1)\n",
        "    return image\n",
        "\n",
        "def upscale_image2(image, width, height, resample=Image.LANCZOS):\n",
        "    # 小数点を丸めてリサイズ\n",
        "    image = image.resize((width, height), resample)\n",
        "    enhancer = ImageEnhance.Sharpness(image)\n",
        "    # enhancerオブジェクトの強調\n",
        "    image = enhancer.enhance(1)\n",
        "    return image\n",
        "\n",
        "def delete_all_files(dir_path):\n",
        "    \"\"\"\n",
        "    フォルダ内のファイルをすべて削除する\n",
        "\n",
        "    Args:\n",
        "        dir_path: フォルダパス\n",
        "    \"\"\"\n",
        "    for file in os.listdir(dir_path):\n",
        "      try:\n",
        "        os.remove(os.path.join(dir_path, file))\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def make_randomized_face(data,gender,me_color=None,chara=None,chara_num=1):\n",
        "    prompt = \"\"\n",
        "\n",
        "    me = pick_rnd(data[\"me\"])\n",
        "    if me_color == None:\n",
        "        me_color = pick_rnd(data[\"me_color\"])\n",
        "    me_add = pick_rnd(data[\"me_add\"])\n",
        "    hana = pick_rnd(data[\"hana\"])\n",
        "    kuchi = pick_rnd(data[\"kuchi\"])\n",
        "    ago = pick_rnd(data[\"ago\"])\n",
        "\n",
        "    #has_add  = random.choices({True,False},weights=[0.25,0.75],k=1)[0]\n",
        "\n",
        "    prompt += set_word(me,1.1)\n",
        "    if me_color is not None:\n",
        "        prompt += set_word(me_color)\n",
        "\n",
        "    prompt += set_word(hana,1.1)\n",
        "    prompt += set_word(kuchi,1.1)\n",
        "    prompt += set_word(ago,1.1)\n",
        "\n",
        "    if chara is not None:\n",
        "        chara_mix = random.sample(data[chara],chara_num)\n",
        "        chara_prompt = \"\"\n",
        "        for mix in chara_mix:\n",
        "          chara_prompt += f\"({mix}),\"\n",
        "        prompt += chara_prompt\n",
        "        # prompt += set_word(pick_rnd[data[chara]],1.1)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def make_randomized_hair(data,color=None):\n",
        "    prompt = \"\"\n",
        "    maegami = pick_rnd(data[\"maegami\"])\n",
        "    usirogami = pick_rnd(data[\"usirogami\"])\n",
        "    nagasa = pick_rnd(data[\"nagasa\"])\n",
        "    yokogami = pick_rnd(data[\"yokogami\"])\n",
        "    hair_type = pick_rnd(data[\"hair_type\"])\n",
        "    hair_color = pick_rnd(data[\"hair_color\"])\n",
        "\n",
        "    form = random.choices([\"random\",\"fix\"],weights=[0.05,0.95],k=1)[0]\n",
        "    has_yokogami = random.choices([True,False],weights=[0.4,0.6],k=1)[0]\n",
        "    has_nagasa = random.choices([True,False],weights=[0.3,0.7],k=1)[0]\n",
        "    has_hair_type = random.choices([True,False],weights=[0.3,0.7],k=1)[0]\n",
        "\n",
        "    prompt += set_word(maegami,1.1)\n",
        "    prompt += set_word(usirogami,1.1)\n",
        "    if has_yokogami:\n",
        "        prompt += set_word(yokogami,1.1)\n",
        "    if has_nagasa:\n",
        "        prompt += set_word(nagasa)\n",
        "    if has_hair_type:\n",
        "        prompt += set_word(hair_type)\n",
        "\n",
        "    if color:\n",
        "        prompt += f\"{color},\"\n",
        "    else:\n",
        "        prompt += set_word(hair_color)\n",
        "    print(\"$$$$ randomized prompt $$$$\")\n",
        "    print(prompt)\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def set_word(word,weight=1.0):\n",
        "    return f\"({word}:{str(weight)}),\"\n",
        "\n",
        "def pick_rnd(my_list):\n",
        "    if not my_list:  # リストが空でないかを確認\n",
        "        return None\n",
        "    return random.choice(my_list)\n",
        "\n",
        "def load_json(filename):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            data = json.load(file)\n",
        "            return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {filename} was not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error: There was a problem decoding the JSON data.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def merge_json(files):\n",
        "    merged_data = {}\n",
        "\n",
        "    for file in files:\n",
        "        temp_data = load_json(file)\n",
        "        merged_data.update(temp_data)  # 辞書を統合\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "def get_rnd_json_files(directory):\n",
        "    try:\n",
        "        # 指定されたディレクトリ内の全ファイルを取得\n",
        "        files = os.listdir(directory)\n",
        "\n",
        "        # 'rnd_'で始まり、拡張子が'.json'のファイルをフィルタリング\n",
        "        rnd_json_files = [os.path.join(directory,file) for file in files if file.startswith('rnd_') and file.endswith('.json')]\n",
        "        return rnd_json_files\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The directory {directory} was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
